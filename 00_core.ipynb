{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jetson-haiku\n",
    "\n",
    "> API details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"Top-level package for jetson-haiku, GPT2 haikubot for Jetson Nano\"\"\"\n",
    "\n",
    "__author__ = \"\"\"LemurTime\"\"\"\n",
    "__email__ = \"software@ananthropicprose.com\"\n",
    "__version__ = \"0.0.1\"\n",
    "\n",
    "#Todo: Update with new GPT2 model\n",
    "#Todo: fine-tine GPT2 output\n",
    "#Todo: add logging\n",
    "#Todo: add Twitter output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#define list of all haikus\n",
    "global finished_haiku\n",
    "global finished_haiku_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import syllapy\n",
    "#import gpt2Pytorch as gp2py\n",
    "#rather, let's just incorporate the gp2pytorch code, for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#Initial arguments go here:\n",
    "\n",
    "GPT2_seed_text=\"Cherry trees in the summer.\"\n",
    "args_nsamples = 1\n",
    "args_batch_size = -1\n",
    "args_length = 1\n",
    "args_unconditional = 0\n",
    "args_temperature = 0.9\n",
    "args_top_k = 40\n",
    "args_quiet = 1\n",
    "verse_input = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#Rather than import GPT2, code here (need to update with new GPT2 model later) \n",
    "#Need to fix this, and import rather than keep GPT2 directory in folder\n",
    "\n",
    "'''\n",
    "    code by TaeHwan Jung(@graykode)\n",
    "    Original Paper and repository here : https://github.com/openai/gpt-2\n",
    "    GPT2 Pytorch Model : https://github.com/huggingface/pytorch-pretrained-BERT\n",
    "'''\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from GPT2.model import (GPT2LMHeadModel)\n",
    "from GPT2.utils import load_weight\n",
    "from GPT2.config import GPT2Config\n",
    "from GPT2.sample import sample_sequence\n",
    "from GPT2.encoder import get_encoder\n",
    "\n",
    "def text_generator(state_dict):\n",
    "   # parser = argparse.ArgumentParser()\n",
    "  #  parser.add_argument(\"--text\", type=str, required=True)\n",
    "   # parser.add_argument(\"--quiet\", type=bool, default=False)\n",
    "   # parser.add_argument(\"--nsamples\", type=int, default=1)\n",
    "   # parser.add_argument('--unconditional', action='store_true', help='If true, unconditional generation.')\n",
    "   # parser.add_argument(\"--batch_size\", type=int, default=-1)\n",
    "   # parser.add_argument(\"--length\", type=int, default=-1)\n",
    "   # parser.add_argument(\"--temperature\", type=float, default=0.7)\n",
    "   # parser.add_argument(\"--top_k\", type=int, default=40)\n",
    "   # args = parser.parse_args()\n",
    "\n",
    "    if args_quiet is False:\n",
    "        print(args)\n",
    "\n",
    "   # if args_batch_size == -1:\n",
    "    args_batch_size = 1\n",
    "    assert args_nsamples % args_batch_size == 0\n",
    "\n",
    "    seed = random.randint(0, 2147483647)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load Model\n",
    "    enc = get_encoder()\n",
    "    config = GPT2Config()\n",
    "    model = GPT2LMHeadModel(config)\n",
    "    model = load_weight(model, state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    #if args_length == -1:\n",
    "    args_length = config.n_ctx // 2\n",
    "   # elif args_length > config.n_ctx:\n",
    "    #    raise ValueError(\"Can't get samples longer than window size: %s\" % config.n_ctx)\n",
    "\n",
    "   # print(args.text)\n",
    "    context_tokens = enc.encode(GPT2_seed_text)\n",
    "\n",
    "    generated = 0\n",
    "    for _ in range(args_nsamples // args_batch_size):\n",
    "        out = sample_sequence(\n",
    "            model=model, length=args_length,\n",
    "            context=context_tokens  if not  args_unconditional else None,\n",
    "            start_token=enc.encoder['<|endoftext|>'] if args_unconditional else None,\n",
    "            batch_size=args_batch_size,\n",
    "            temperature=args_temperature, top_k=args_top_k, device=device\n",
    "        )\n",
    "        out = out[:, len(context_tokens):].tolist()\n",
    "        for i in range(args_batch_size):\n",
    "            generated += 1\n",
    "            text = enc.decode(out[i])\n",
    "            if args_quiet is False:\n",
    "                print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "            global GPT2_output\n",
    "            GPT2_output = text\n",
    "            print(text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if os.path.exists('gpt2-pytorch_model.bin'):\n",
    "        state_dict = torch.load('gpt2-pytorch_model.bin', map_location='cpu' if not torch.cuda.is_available() else None)\n",
    "     #   text_generator(state_dict)\n",
    "    else:\n",
    "        print('Please download gpt2-pytorch_model.bin')\n",
    "        sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "##General verse_gen - input is input, number of syllables required\n",
    "\n",
    "def verse_gen(verse_input, syllable_length):\n",
    "    global verse_words\n",
    "    global verse_string\n",
    "    global verse_count\n",
    "    global verse_syllable_count\n",
    "    \n",
    "    global verse_one_string\n",
    "\n",
    "#Go to first whitespace, count syllables.  Continue until \"syllable_length\" syllables.  If over required amount syllables try with new input.\n",
    "#initialize counter\n",
    "    y=0\n",
    "    x=1\n",
    "    verse_syllable_count=0\n",
    "\n",
    "#Split to remove whitespace\n",
    "    verse_words=verse_input.split(' ')\n",
    "\n",
    "    while verse_syllable_count < syllable_length:\n",
    "        print(\"Adding next word to the string\")\n",
    "\n",
    "#Put the first word in a string\n",
    "        verse_string=' '.join(verse_words[y:x])\n",
    "\n",
    "#Count the syllables\n",
    "        verse_syllable_count = syllapy.count(verse_string)\n",
    "    \n",
    "#increment x\n",
    "        x=x+1\n",
    "\n",
    "#Get new input if the words don't make 5 syllables\n",
    "#        if verse_syllable_count > syllable_length:\n",
    " #           print(\"Need new input\")\n",
    "  #          text_generator(state_dict)\n",
    "   #         verse_input = GPT2_output\n",
    "    #        verse_gen(verse_input, syllable_length)\n",
    "        \n",
    "#If the words make 5 syllables, check for period or comma at the end of it.  Use if so, get new input if not       \n",
    "     #   if verse_syllable_count == syllable_length:\n",
    "          #  if verse_string[-1] == \".\" or verse_string[-1] == \",\":\n",
    "           #     print(verse_string)\n",
    "        #    else:\n",
    "         #       print(\"Need input ending with punctuation\")\n",
    "      #         verse_gen(verse_input, syllable_length)\n",
    "            \n",
    "        \n",
    "        \n",
    "## New way:  go down the input to look for haiku-able phrases.  If not, get new input\n",
    "\n",
    "        if verse_syllable_count == syllable_length:\n",
    "            print(verse_string)\n",
    "            return verse_string\n",
    "    \n",
    "        if verse_syllable_count > syllable_length:\n",
    "        #reinitialize the string and keep going\n",
    "            print(\"Moving up in string\")\n",
    "            print(verse_string)\n",
    "            \n",
    "            #reinitialize verse_string\n",
    "            verse_string=\"\"\n",
    "            verse_syllable_count=0\n",
    "            y=x-1\n",
    "            \n",
    "            #verse_gen(verse_input, syllable_length)\n",
    "\n",
    "       \n",
    "      \n",
    "\n",
    "#END OF VERSE ONE GEN    \n",
    "\n",
    "\n",
    "\n",
    "##Now we will take verse_one_string, and add it to our haiku\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "## Code to run the module\n",
    "\n",
    "def haiku_gen():\n",
    "    text_generator(state_dict)\n",
    "\n",
    "#Code to generate verse 1:\n",
    "    verse_string = \"\"\n",
    "    verse_input = GPT2_output\n",
    "    syllable_length = 5\n",
    "    verse_one_string=verse_gen(verse_input, syllable_length)\n",
    "    \n",
    "#Code to generate verse 2:\n",
    "    verse_string = \"\"\n",
    "    GPT2_seed_text = verse_one_string\n",
    "    text_generator(state_dict)\n",
    "    verse_input = GPT2_output\n",
    "    syllable_length = 7\n",
    "    verse_two_string=verse_gen(verse_input, syllable_length)\n",
    "\n",
    "#Code to generate verse 3:\n",
    "    verse_string = \"\"\n",
    "    GPT2_seed_text = verse_one_string\n",
    "    text_generator(state_dict)\n",
    "    verse_input = GPT2_output\n",
    "    syllable_length=5\n",
    "    verse_three_string=verse_gen(verse_input, syllable_length)\n",
    "\n",
    "#Print finished haiku\n",
    "    print(\"Here is the haiku:\")\n",
    "\n",
    "#Print finished haiku\n",
    " \n",
    "    finished_haiku=''\n",
    "    finished_haiku='\\n'.join([verse_one_string,verse_two_string,verse_three_string])\n",
    "    print(finished_haiku)\n",
    "    \n",
    "    #Add finished haiku to a list\n",
    "    f = open(\"haikulist.txt\", \"a\")\n",
    "    f.write(\"\\n\\n\"+finished_haiku)\n",
    "    f.close()\n",
    "\n",
    "    #Put verse2 in as GPT seedtext seed\n",
    "    f = open(\"haikuseed.txt\", \"w\")\n",
    "    f.write(verse_two_string)\n",
    "    f.close()\n",
    "    \n",
    "    from IPython.display import Audio\n",
    "\n",
    "    wave = np.sin(2*np.pi*400*np.arange(10000*2)/10000)\n",
    "    Audio(wave, rate=30000, autoplay=True)\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:48<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " At least he tried to take out each of the three dogs, to try and get her to eat at least one of them.\n",
      "\n",
      "\"Mighty!\" they shouted and were screaming.\n",
      "\n",
      "But the animal didn't move or move.\n",
      "\n",
      "He pulled out his dog's collar and the dogs started to fight, and he was done, the dog said.\n",
      "\n",
      "Then, one of the dogs ran after her, and he grabbed her, and she ran away. A dog in a car with a dog in the back gave her a bit of pain for several days while they battled it.\n",
      "\n",
      "\"You didn't know she had that?\" the lady in the car said.\n",
      "\n",
      "Yes, I did.\n",
      "\n",
      "We all knew the dog was one of the best dogs of the past year. It is a very important breed and there is a lot that it deserves, for it was one of the best breeds of dogs I know, and I've never seen a girl get so scared in her life. In the whole family, she's the only one who has ever gotten so scared.\n",
      "\n",
      "We all know how it felt to fight back when she was attacked. It was a big deal, really, all the time. It was a thing we did. It was the ultimate act of aggression. We didn't know what it was. Maybe she had a problem. She had a problem with fear, too. Maybe some other people had a problem with fear.\n",
      "\n",
      "That is the story that I am trying to tell you.\n",
      "\n",
      "You will hear the story.\n",
      "\n",
      "She was on the porch, walking around. She had no face, no nose. And she was wearing a tuxedo. And she said, \"Gee, I'll see you on Tuesday,\" so everyone knew who was behind her. And so she was sitting there with the shirt up, and she looked at me and she said, \"Oh my God, I couldn't hold my breath, you're not going to be able to do something like that. I don't care. You're so stupid.\"\n",
      "\n",
      "But a dog is not human. He is our own dog and we're very comfortable with him. The only things he has that I can respect are, we give him the most beautiful gifts, and when he comes to us, we give him a very loving home and a wonderful home with him in it.\n",
      "\n",
      "And I never did say the word \"dogs.\"\n",
      "\n",
      "It was funny, because I always\n",
      "Adding next word to the string\n",
      "Adding next word to the string\n",
      "Adding next word to the string\n",
      "Adding next word to the string\n",
      "Adding next word to the string\n",
      "Adding next word to the string\n",
      " At least he tried to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:45<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We got off and walked and walked and walked, so he didn't have to do anything; he didn't think of it as a serious matter. He just went. And the thing about the men he had on his body was that when you see people's faces, or you go into the office and look up at them and there's a lot of the stuff from their lives that they're still in, when you're reading their books, you look for the truth, and the truth doesn't find a way. So he looked for it. The story that he found was what happened on his own.\n",
      "\n",
      "C.J.: Did the people you had met in Mexico tell you anything about your family from the beginning?\n",
      "\n",
      "D.C.: I think it has to do with a part that we all know. A person might have gone to the cemetery in the middle of the night and that's in some weird way in order to avoid people who were with the family. We know from the family's story that all of the Mexicans were there in the late afternoon and that there always was a lot of drugs in there. People were always there and sometimes he would ask if they could pass a piece of paper around and pass it around. And we don't have any recollection of that because we are in a country when there is so many drugs, and he would always ask, \"Why is it that this place is so nice?\" But in Mexico people do that to you. They were going into the cemetery, and they would pass the paper over. They had been there for a long time.\n",
      "\n",
      "C.J.: And how did you know how dangerous the drugs could be?\n",
      "\n",
      "D.C.: In the Mexican prison system and in the prisons in Mexico, everyone had an ID that they used to pass in the cemetery and on how they were going about their duties. That was one of the great things about Mexico. The only exception was the jail where we were held for almost three weeks. And then, as you know, there was no one in the prison in Mexico who could pass an ID because we were separated so much. And then, when somebody passes an ID, they are only allowed to get that ID for a short period of time, just to get a picture of the person.\n",
      "\n",
      "We had to deal with that for nearly four years. We had to do every step of the way, because there were so many other people around us who could take pictures. And all of a sudden every day we\n",
      "Adding next word to the string\n",
      "Adding next word to the string\n",
      "Adding next word to the string\n",
      "Adding next word to the string\n",
      "Adding next word to the string\n",
      "Adding next word to the string\n",
      "Adding next word to the string\n",
      " We got off and walked and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 508/512 [00:45<00:00, 10.87it/s]"
     ]
    }
   ],
   "source": [
    "#export\n",
    "\n",
    "##Run the module:\n",
    "\n",
    "#Initial arguments go here:\n",
    "from IPython.display import Audio\n",
    "GPT2_seed_text=\"Gorillas in the mist.\"\n",
    "args_nsamples = 1\n",
    "args_batch_size = -1\n",
    "args_length = 1\n",
    "args_unconditional = 0\n",
    "args_temperature = 0.9\n",
    "args_top_k = 40\n",
    "args_quiet = 1\n",
    "verse_input = \"\"\n",
    "z = 0\n",
    "\n",
    "while z < 100:\n",
    "    \n",
    "    haiku_gen()\n",
    "    f = open(r\"haikuseed.txt\")\n",
    "    GPT2_seed_text=f.readline()\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    #Beep after each iteration\n",
    "    wave = np.sin(2*np.pi*400*np.arange(10000*2)/10000)\n",
    "    Audio(wave, rate=30000, autoplay=True)\n",
    "    z+1\n",
    "    \n",
    "#Beep when all done\n",
    "\n",
    "wave = np.sin(2*np.pi*400*np.arange(10000*2)/10000)\n",
    "Audio(wave, rate=40000, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nbdev.export import *\n",
    "#notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo:\n",
    "#Training feature - what is a good/bad haiku\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "##Todo: make ananthropic\n",
    "#Remove: wordlist indicating persons or personification (I, his, hers, mine, ours, who)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haiku",
   "language": "python",
   "name": "haiku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
