{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jetson-haiku\n",
    "\n",
    "> API details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"Top-level package for jetson-haiku, GPT2 haikubot for Jetson Nano\"\"\"\n",
    "\n",
    "__author__ = \"\"\"LemurTime\"\"\"\n",
    "__email__ = \"software@ananthropicprose.com\"\n",
    "__version__ = \"0.0.1\"\n",
    "\n",
    "#Todo: Update with new GPT2 model\n",
    "#Todo: fine-tine GPT2 output\n",
    "#Todo: add logging\n",
    "#Todo: add Twitter output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#define list of all haikus\n",
    "global finished_haiku\n",
    "global finished_haiku_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import syllapy\n",
    "#import gpt2Pytorch as gp2py\n",
    "#rather, let's just incorporate the gp2pytorch code, for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#Initial arguments go here:\n",
    "\n",
    "GPT2_seed_text=\"Cherry trees in the summer.\"\n",
    "args_nsamples = 1\n",
    "args_batch_size = -1\n",
    "args_length = 1\n",
    "args_unconditional = 0\n",
    "args_temperature = 0.9\n",
    "args_top_k = 40\n",
    "args_quiet = 1\n",
    "verse_input = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#Rather than import GPT2, code here (need to update with new GPT2 model later) \n",
    "#Need to fix this, and import rather than keep GPT2 directory in folder\n",
    "\n",
    "'''\n",
    "    code by TaeHwan Jung(@graykode)\n",
    "    Original Paper and repository here : https://github.com/openai/gpt-2\n",
    "    GPT2 Pytorch Model : https://github.com/huggingface/pytorch-pretrained-BERT\n",
    "'''\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from GPT2.model import (GPT2LMHeadModel)\n",
    "from GPT2.utils import load_weight\n",
    "from GPT2.config import GPT2Config\n",
    "from GPT2.sample import sample_sequence\n",
    "from GPT2.encoder import get_encoder\n",
    "\n",
    "def text_generator(state_dict):\n",
    "   # parser = argparse.ArgumentParser()\n",
    "  #  parser.add_argument(\"--text\", type=str, required=True)\n",
    "   # parser.add_argument(\"--quiet\", type=bool, default=False)\n",
    "   # parser.add_argument(\"--nsamples\", type=int, default=1)\n",
    "   # parser.add_argument('--unconditional', action='store_true', help='If true, unconditional generation.')\n",
    "   # parser.add_argument(\"--batch_size\", type=int, default=-1)\n",
    "   # parser.add_argument(\"--length\", type=int, default=-1)\n",
    "   # parser.add_argument(\"--temperature\", type=float, default=0.7)\n",
    "   # parser.add_argument(\"--top_k\", type=int, default=40)\n",
    "   # args = parser.parse_args()\n",
    "\n",
    "    if args_quiet is False:\n",
    "        print(args)\n",
    "\n",
    "   # if args_batch_size == -1:\n",
    "    args_batch_size = 1\n",
    "    assert args_nsamples % args_batch_size == 0\n",
    "\n",
    "    seed = random.randint(0, 2147483647)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load Model\n",
    "    enc = get_encoder()\n",
    "    config = GPT2Config()\n",
    "    model = GPT2LMHeadModel(config)\n",
    "    model = load_weight(model, state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    #if args_length == -1:\n",
    "    args_length = config.n_ctx // 2\n",
    "   # elif args_length > config.n_ctx:\n",
    "    #    raise ValueError(\"Can't get samples longer than window size: %s\" % config.n_ctx)\n",
    "\n",
    "   # print(args.text)\n",
    "    context_tokens = enc.encode(GPT2_seed_text)\n",
    "\n",
    "    generated = 0\n",
    "    for _ in range(args_nsamples // args_batch_size):\n",
    "        out = sample_sequence(\n",
    "            model=model, length=args_length,\n",
    "            context=context_tokens  if not  args_unconditional else None,\n",
    "            start_token=enc.encoder['<|endoftext|>'] if args_unconditional else None,\n",
    "            batch_size=args_batch_size,\n",
    "            temperature=args_temperature, top_k=args_top_k, device=device\n",
    "        )\n",
    "        out = out[:, len(context_tokens):].tolist()\n",
    "        for i in range(args_batch_size):\n",
    "            generated += 1\n",
    "            text = enc.decode(out[i])\n",
    "            if args_quiet is False:\n",
    "                print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "            global GPT2_output\n",
    "            GPT2_output = text\n",
    "            print(text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if os.path.exists('gpt2-pytorch_model.bin'):\n",
    "        state_dict = torch.load('gpt2-pytorch_model.bin', map_location='cpu' if not torch.cuda.is_available() else None)\n",
    "     #   text_generator(state_dict)\n",
    "    else:\n",
    "        print('Please download gpt2-pytorch_model.bin')\n",
    "        sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "##General verse_gen - input is input, number of syllables required\n",
    "\n",
    "def verse_gen(verse_input, syllable_length):\n",
    "    global verse_words\n",
    "    global verse_string\n",
    "    global verse_count\n",
    "    global verse_syllable_count\n",
    "    \n",
    "    global verse_one_string\n",
    "\n",
    "#Go to first whitespace, count syllables.  Continue until \"syllable_length\" syllables.  If over required amount syllables try with new input.\n",
    "#initialize counter\n",
    "    y=0\n",
    "    x=1\n",
    "    verse_syllable_count=0\n",
    "\n",
    "#Split to remove whitespace\n",
    "    verse_words=verse_input.split(' ')\n",
    "\n",
    "    while verse_syllable_count < syllable_length:\n",
    "        print(\"Adding next word to the string\")\n",
    "\n",
    "#Put the first word in a string\n",
    "        verse_string=' '.join(verse_words[y:x])\n",
    "\n",
    "#Count the syllables\n",
    "        verse_syllable_count = syllapy.count(verse_string)\n",
    "    \n",
    "#increment x\n",
    "        x=x+1\n",
    "\n",
    "#Get new input if the words don't make 5 syllables\n",
    "#        if verse_syllable_count > syllable_length:\n",
    " #           print(\"Need new input\")\n",
    "  #          text_generator(state_dict)\n",
    "   #         verse_input = GPT2_output\n",
    "    #        verse_gen(verse_input, syllable_length)\n",
    "        \n",
    "#If the words make 5 syllables, check for period or comma at the end of it.  Use if so, get new input if not       \n",
    "     #   if verse_syllable_count == syllable_length:\n",
    "          #  if verse_string[-1] == \".\" or verse_string[-1] == \",\":\n",
    "           #     print(verse_string)\n",
    "        #    else:\n",
    "         #       print(\"Need input ending with punctuation\")\n",
    "      #         verse_gen(verse_input, syllable_length)\n",
    "            \n",
    "        \n",
    "        \n",
    "## New way:  go down the input to look for haiku-able phrases.  If not, get new input\n",
    "\n",
    "        if verse_syllable_count == syllable_length:\n",
    "            print(verse_string)\n",
    "            return verse_string\n",
    "    \n",
    "        if verse_syllable_count > syllable_length:\n",
    "        #reinitialize the string and keep going\n",
    "            print(\"Moving up in string\")\n",
    "            print(verse_string)\n",
    "            \n",
    "            #reinitialize verse_string\n",
    "            verse_string=\"\"\n",
    "            verse_syllable_count=0\n",
    "            y=x-1\n",
    "            \n",
    "            #verse_gen(verse_input, syllable_length)\n",
    "\n",
    "       \n",
    "      \n",
    "\n",
    "#END OF VERSE ONE GEN    \n",
    "\n",
    "\n",
    "\n",
    "##Now we will take verse_one_string, and add it to our haiku\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "## Code to run the module\n",
    "\n",
    "def haiku_gen():\n",
    "    text_generator(state_dict)\n",
    "\n",
    "#Code to generate verse 1:\n",
    "    verse_string = \"\"\n",
    "    verse_input = GPT2_output\n",
    "    syllable_length = 5\n",
    "    verse_one_string=verse_gen(verse_input, syllable_length)\n",
    "    \n",
    "#Code to generate verse 2:\n",
    "    verse_string = \"\"\n",
    "    GPT2_seed_text = verse_one_string\n",
    "    text_generator(state_dict)\n",
    "    verse_input = GPT2_output\n",
    "    syllable_length = 7\n",
    "    verse_two_string=verse_gen(verse_input, syllable_length)\n",
    "\n",
    "#Code to generate verse 3:\n",
    "    verse_string = \"\"\n",
    "    GPT2_seed_text = verse_one_string\n",
    "    text_generator(state_dict)\n",
    "    verse_input = GPT2_output\n",
    "    syllable_length=5\n",
    "    verse_three_string=verse_gen(verse_input, syllable_length)\n",
    "\n",
    "#Print finished haiku\n",
    "    print(\"Here is the haiku:\")\n",
    "\n",
    "#Print finished haiku\n",
    " \n",
    "    finished_haiku=''\n",
    "    finished_haiku='\\n'.join([verse_one_string,verse_two_string,verse_three_string])\n",
    "    print(finished_haiku)\n",
    "    \n",
    "    #Add finished haiku to a list\n",
    "    f = open(\"haikulist.txt\", \"a\")\n",
    "    f.write(\"\\n\\n\"+finished_haiku)\n",
    "    f.close()\n",
    "    \n",
    "    #Place finished haiku in an input for GUI (clear it out first)\n",
    "    f = open(\"latesthaiku.txt\", \"w\")\n",
    "    f.write(finished_haiku)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    #Put verse2 in as GPT seedtext seed\n",
    "    f = open(\"haikuseed.txt\", \"w\")\n",
    "    f.write(verse_two_string)\n",
    "    f.close()\n",
    "    \n",
    "    from IPython.display import Audio\n",
    "\n",
    "    wave = np.sin(2*np.pi*400*np.arange(10000*2)/10000)\n",
    "    Audio(wave, rate=30000, autoplay=True)\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "##Run the module:\n",
    "\n",
    "#Initial arguments go here:\n",
    "from IPython.display import Audio\n",
    "GPT2_seed_text=\"Gorillas in the mist.\"\n",
    "args_nsamples = 1\n",
    "args_batch_size = -1\n",
    "args_length = 1\n",
    "args_unconditional = 0\n",
    "args_temperature = 0.9\n",
    "args_top_k = 40\n",
    "args_quiet = 1\n",
    "verse_input = \"\"\n",
    "z = 0\n",
    "\n",
    "while z < 100:\n",
    "    \n",
    "    haiku_gen()\n",
    "    f = open(r\"haikuseed.txt\")\n",
    "    GPT2_seed_text=f.readline()\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    #Beep after each iteration\n",
    "    wave = np.sin(2*np.pi*400*np.arange(10000*2)/10000)\n",
    "    Audio(wave, rate=30000, autoplay=True)\n",
    "    z+1\n",
    "    \n",
    "#Beep when all done\n",
    "\n",
    "wave = np.sin(2*np.pi*400*np.arange(10000*2)/10000)\n",
    "Audio(wave, rate=40000, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nbdev.export import *\n",
    "#notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo:\n",
    "#Training feature - what is a good/bad haiku\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "##Todo: make ananthropic\n",
    "#Remove: wordlist indicating persons or personification (I, his, hers, mine, ours, who)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haiku",
   "language": "python",
   "name": "haiku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
